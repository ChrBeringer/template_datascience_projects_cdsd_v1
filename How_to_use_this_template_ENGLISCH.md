# template_datascience_projects_cdsd_v1
This is a template for scientific data science projects based on the Cookiecutter Data Science (CDSD) structure and common best practices.
A standardized, pre-structured project template developed to ensure the reproducibility and organization of data science projects.

# Key Guiding Principles
1. Strict Data Separation: The data/ folder is subdivided into 01-raw, 02-processed, etc. This is the most important contribution to reproducibility, as it clarifies which data version is used for which pipeline.
2. Separation of Exploration and Production: Notebooks (notebooks/) are separate from the modular, testable code (src/).
3. Standardized Entrypoints: The scripts in entrypoint/ enable easy automation.

Resulting in:
- Reproducibility: "Every step, from raw data to final analysis, is traceable."
- Standardization: "New team members can immediately orient themselves, as the structure is always the same."
- Modularity: "The code is divided into small, testable units (modules in src/)."
- Organization: "Documents, configuration, code, and data are logically separated."
- Automation: Through clear entrypoints and dependency files, the entire process can be automated (CI/CD).

## üìÅ Detailed Project Structure

The folder structure strictly separates code, data, and configuration to maximize clarity and modularity.

| Folder | Purpose | Main Content |
| :--- | :--- | :--- |
| **`config/`** | **Configuration** | Static parameters, hyperparameters, and paths (`.yaml`, `.json`). |
| **`data/`** | **Data Versioning** | All datasets, separated by processing stage. |
| **`entrypoint/`** | **Startup Scripts** | Main scripts for orchestrating the pipelines (`train.py`, `inference.py`). |
| **`notebooks/`** | **Exploration (EDA)** | Jupyter Notebooks for iterative analysis and prototyping. |
| **`src/`** | **Source Code** | Modular, testable code and reusable pipelines (`utils.py`, `pipelines/`). |
| **`tests/`** | **Quality Assurance** | Unit tests to verify the correctness of the code in `src/`. |

### üíæ Detailed Data Workflow: The `data/` Folder

The `data/` folder serves the **strict separation of data assets** based on their processing state and is the foundation for reproducibility.

| Subfolder | Content / Function | Rules and Purpose |
| :--- | :--- | :--- |
| **`01-raw/`** | **Raw Data (Original)** | Contains the **original, unmodified source data**. **Rule:** These files must **NEVER** be changed manually or by scripts after the initial addition. They serve as the single source of truth. |
| **`02-processed/`** | **Cleaned Data** | Contains datasets that have undergone the first phase of **cleaning** (e.g., handling missing values, formatting, data type correction). **Rule:** This data is generated by scripts in `src/pipelines/` from `01-raw/`. |
| **`03-features/`** | **Feature Sets** | Contains the **final feature matrices** that serve directly as input for the Machine Learning model. All feature engineering steps are completed here (e.g., scaling, encoding). |
| **`04-predictions/`** | **Model Results** | Contains the **output** of the trained models. This includes final prediction values, reports, or metric files generated by the model during the production run. |

# How should the structure be used?

## config/
Configuration and Parameters. Stores all static settings that can change based on the environment or experiment, without altering the code. Store *.yaml or *.json files here for paths, hyperparameters, seed values, etc. The scripts in entrypoint/ and src/ read these files.

## /data
Organizes all data by its processing state (raw data to final results) to ensure traceability. Never modify data in 01-raw/! The scripts in src/pipelines/ generate the subsequent stages (02-processed/, 03-features/).

## /entrypoint
Application Entrypoints. Serves as the interface for executing the project's main tasks. Contains no logic itself, but orchestrates the pipelines from src/. These are the scripts you call directly from the command line (e.g., python entrypoint/train.py). They are used for automation (CI/CD).

## notebooks/
Exploration and Prototyping. Used for iterative data analysis (EDA), visualizations, and quick testing of logic. Store Jupyter/IPython Notebooks here. Try to migrate proven logic to src/ as quickly as possible.

## src/
Production-ready Source Code. Contains all modular functions, classes, and pipelines that represent the core logic of the project. Every file in src/ should be importable and testable. The entrypoint/ folder calls these modules.

## tests/
Quality Assurance. Includes unit tests and integration tests to verify the correctness of the code in src/. Run tests before every deployment (e.g., with pytest). This ensures code quality and prevents regressions.

## üíæ Data Workflow: The `data/` Structure

The folders under `data/` serve to version and protect data integrity. Data flows sequentially through these folders, controlled by the scripts in `src/pipelines/`.

| Folder | Function | Git Status |
| :--- | :--- | :--- |
| **`01-raw/`** | **Original Data** | The **unmodified** source data. NEVER modify manually. |
| **`02-processed/`** | **Cleaned Data** | Data after cleaning (missing values, formatting) ‚Äì ready for feature engineering. |
| **`03-features/`** | **Feature Sets** | Data containing all necessary features and encodings ‚Äì direct input for the model. |
| **`04-predictions/`** | **Model Results** | The output of the inference scripts (`inference.py`), such as prediction values or metrics. |

### üö® Important Note: Data Versioning and Storage

Since files in `data/` are often very large, they are generally **ignored** in the `.gitignore` file.

**For managing and versioning the data assets, it is recommended:**

1.  **Placeholders:** The **`.gitkeep`** files in each subfolder ensure that the empty folder structure is tracked in Git.
2.  **DVC (Data Version Control):** Use DVC to store **metadata** (hash values) of the large files in Git, while the actual data resides in a dedicated **Remote Storage** (e.g., S3, Google Cloud Storage). This enables the reproducibility of every data version without bloating the Git repository.

---

## üõ† Development and Tests

* **Run Tests:**
    ```bash
    python -m pytest tests/
    ```
* **Code in Notebooks:** Execute exploratory code in `notebooks/`. Once the logic is mature, migrate it to the modules in `src/` (e.g., in `src/pipelines/data_processing.py`) so that it becomes testable and reproducible.
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ unit/                    # Unit tests for src/
        ‚îî‚îÄ‚îÄ test_utils.py
