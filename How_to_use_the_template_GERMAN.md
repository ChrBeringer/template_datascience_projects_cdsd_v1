# template_datascience_projects_cdsd_v1
Dies ist ein template f√ºr wissenschaftliche Data-Science-Projekte basierend auf der Cookie-Cutter-Struktur (CDSD) und g√§ngiger Best-Practice.
Eine standardisierte, vorstrukturierte Projektvorlage (Template), die entwickelt wurde, um die Reproduzierbarkeit und Organisation von Data-Science-Projekten zu gew√§hrleisten.

# Wichtige Grundprinzipien
1. Strikte Trennung von Daten: Der data/-Ordner wird in 01-raw, 02-processed, etc. unterteilt. Dies ist der wichtigste Beitrag zur Reproduzierbarkeit, da klar ist, welche Datenversion f√ºr welche Pipeline verwendet wird.
2. Trennung von Exploration und Produktion: Notebooks (notebooks/) sind getrennt vom modularen, testbaren Code (src/).
3. Standardisierte Entrypoints: Die Skripte in entrypoint/ erm√∂glichen eine einfache Automatisierung.

Dadurch:
- Reproduzierbarkeit,"Jeder Schritt, von den Rohdaten bis zur finalen Analyse, ist nachvollziehbar."
- Standardisierung,"Neue Teammitglieder finden sich sofort zurecht, da die Struktur immer gleich ist."
- Modularit√§t,"Der Code ist in kleine, testbare Einheiten (Module in src/) unterteilt."
- Organisation,"Dokumente, Konfiguration, Code und Daten sind logisch getrennt."
- Automatisierung,Durch klare Entrypoints und Abh√§ngigkeitsdateien l√§sst sich der gesamte Prozess automatisieren (CI/CD).

## üìÅ Projektstruktur im Detail

Die Ordnerstruktur trennt Code, Daten und Konfiguration strikt, um Klarheit und Modularit√§t zu maximieren.

| Ordner | Zweck | Hauptinhalt |
| :--- | :--- | :--- |
| **`config/`** | **Konfiguration** | Statische Parameter, Hyperparameter, und Pfade (`.yaml`, `.json`). |
| **`data/`** | **Datenversionierung** | Alle Datasets, getrennt nach Verarbeitungsstadium. |
| **`entrypoint/`** | **Start-Skripte** | Hauptskripte zur Orchestrierung der Pipelines (`train.py`, `inference.py`). |
| **`notebooks/`** | **Exploration (EDA)** | Jupyter Notebooks f√ºr iterative Analyse und Prototyping. |
| **`src/`** | **Quellcode** | Modularer, testbarer Code und wiederverwendbare Pipelines (`utils.py`, `pipelines/`). |
| **`tests/`** | **Qualit√§tssicherung** | Unit-Tests zur √úberpr√ºfung der Korrektheit des Codes in `src/`. |

### üíæ Daten-Workflow im Detail: Der `data/`-Ordner

Der Ordner `data/` dient der **strengen Trennung der Datenbest√§nde** nach ihrem Verarbeitungszustand und ist die Grundlage f√ºr die Reproduzierbarkeit.

| Unterordner | Inhalt / Funktion | Regeln und Zweck |
| :--- | :--- | :--- |
| **`01-raw/`** | **Rohdaten (Original)** | Enth√§lt die **urspr√ºnglichen, unver√§nderten Quelldaten**. **Regel:** Diese Dateien d√ºrfen nach dem ersten Hinzuf√ºgen **NIEMALS** ver√§ndert werden. Sie dienen als die einzige Quelle der Wahrheit. |
| **`02-processed/`** | **Bereinigte Daten** | Enth√§lt Datens√§tze, die die erste Phase der **Bereinigung** (fehlende Werte, Formatierung) durchlaufen haben. **Regel:** Diese Daten werden von Skripten in `src/pipelines/` aus `01-raw/` generiert. |
| **`03-features/`** | **Feature-Sets** | Enth√§lt die **fertigen Feature-Matrizen**, die unmittelbar als Eingabe (Input) f√ºr das Machine-Learning-Modell dienen. Alle Feature-Engineering-Schritte sind hier abgeschlossen (z.B. Skalierung, Encoding). |
| **`04-predictions/`** | **Modellergebnisse** | Enth√§lt die **Ausgabe** der trainierten Modelle. Dazu geh√∂ren die finalen Vorhersagewerte, Berichte oder Metrik-Dateien, die das Modell erzeugt hat. |

# Wie sollte die Struktur verwendet werden?

## config/
Konfiguration und Parameter. Speichert alle statischen Einstellungen, die sich je nach Umgebung oder Experiment √§ndern k√∂nnen, ohne den Code zu ber√ºhren.Speichern Sie hier *.yaml oder *.json Dateien f√ºr Pfade, Hyperparameter, Seed-Werte etc. Die Skripte in entrypoint/ und src/ lesen diese Dateien ein.data 

## /data 
Organisiert alle Daten nach ihrem Verarbeitungszustand (Rohdaten bis Endergebnisse), um die Nachvollziehbarkeit zu gew√§hrleisten. Niemals Daten in 01-raw/ ver√§ndern! Die Skripte in src/pipelines/ generieren die nachfolgenden Stufen (02-processed/, 03-features/).

## /entrypoint
Anwendungsstartpunkte. Dient als Schnittstelle f√ºr das Ausf√ºhren der Hauptaufgaben des Projekts. Enth√§lt keine Logik, sondern orchestriert die Pipelines aus src/.Dies sind die Skripte, die Sie direkt √ºber die Kommandozeile aufrufen (z.B. python entrypoint/train.py). Sie werden f√ºr die Automatisierung (CI/CD) verwendet.

## notebooks/
Exploration und Prototyping. Wird f√ºr iterative Datenanalyse (EDA), Visualisierungen und schnelles Testen von Logik verwendet.Speichern Sie hier Jupyter/IPython Notebooks. Versuchen Sie, Logik, die sich bew√§hrt hat, schnellstm√∂glich nach src/ zu migrieren.

## src/
Produktionsreifer Quellcode. Enth√§lt alle modularen Funktionen, Klassen und Pipelines, die die Kernlogik des Projekts abbilden.Jede Datei in src/ sollte importierbar und testbar sein. Der entrypoint/ Ordner ruft diese Module auf.

## tests/
Qualit√§tssicherung. Beinhaltet Unit-Tests und Integrationstests, um die Korrektheit des Codes in src/ zu √ºberpr√ºfen. F√ºhren Sie Tests vor jedem Deployment aus (z.B. mit pytest). Dies sichert die Code-Qualit√§t und beugt Regressionen vor.

## üíæ Daten-Workflow: Die `data/`-Struktur

Die Ordner unter `data/` dienen der Versionierung und dem Schutz der Datenintegrit√§t. Die Daten flie√üen sequenziell durch diese Ordner, gesteuert durch die Skripte in `src/pipelines/`.

| Ordner | Funktion | Git-Status |
| :--- | :--- | :--- |
| **`01-raw/`** | **Originaldaten** | Die **unver√§nderlichen** Quelldaten. NIEMALS manuell ver√§ndern. |
| **`02-processed/`** | **Bereinigte Daten** | Daten nach Bereinigung (Fehlwerte, Formatierung) ‚Äì bereit f√ºr das Feature Engineering. |
| **`03-features/`** | **Feature-Sets** | Daten, die alle ben√∂tigten Features und Codierungen enthalten ‚Äì direkter Input f√ºr das Modell. |
| **`04-predictions/`** | **Modellergebnisse** | Die Ausgabe der Inferenz-Skripte (`inference.py`) wie Vorhersagewerte oder Metriken. |

### üö® Wichtiger Hinweis: Datenversionierung und Speicher

Da die Dateien in `data/` oft sehr gro√ü sind, werden sie in der Regel in der `.gitignore` **ignoriert**.

* Um die leere Ordnerstruktur dennoch in Git zu verfolgen, enthalten alle Unterordner die Platzhalterdatei **`.gitkeep`**.
* F√ºr die Versionierung der **tats√§chlichen Daten** verwenden Sie bitte **DVC (Data Version Control)**.


Da gro√üe Datens√§tze die Performance von Git stark beeintr√§chtigen w√ºrden, werden die Dateien in `data/` in der Regel von der `.gitignore` **ignoriert**.

**Zur Verwaltung und Versionierung der Datenbest√§nde wird empfohlen:**

1.  **Platzhalter:** Die Dateien **`.gitkeep`** in jedem Unterordner stellen sicher, dass die leere Ordnerstruktur in Git verfolgt wird.
2.  **DVC (Data Version Control):** Verwenden Sie DVC, um **Metadaten** (Hash-Werte) der gro√üen Dateien in Git zu speichern, w√§hrend die eigentlichen Daten in einem dedizierten **Remote Storage** (z.B. S3, Google Cloud Storage) liegen. Dies erm√∂glicht die Reproduzierbarkeit jeder Datenversion, ohne das Git-Repository aufzubl√§hen.

---

## üõ† Entwicklung und Tests

* **Tests ausf√ºhren:**
    ```bash
    python -m pytest tests/
    ```
* **Code in Notebooks:** F√ºhren Sie explorativen Code in `notebooks/` aus. Sobald die Logik ausgereift ist, migrieren Sie diese in die Module in `src/` (z.B. in `src/pipelines/data_processing.py`), damit sie testbar und reproduzierbar wird.
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ unit/                    # Unit-Tests f√ºr src/
        ‚îî‚îÄ‚îÄ test_utils.py
